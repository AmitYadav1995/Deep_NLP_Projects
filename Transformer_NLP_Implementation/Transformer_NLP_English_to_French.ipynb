{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_for_NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6SFl33Pv9zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" DataSet Description \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvVesQU_IbUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Importing the libraries \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4tV-H7Bu_M3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Data Pre-Processing :\n",
        "\n",
        "1. Loading the files \"\"\"\n",
        "\n",
        "with open(\"/content/drive/My Drive/Udemy Lectures/DeepNLP/Transformer/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Udemy Lectures/DeepNLP/Transformer/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "with open(\"/content/drive/My Drive/Udemy Lectures/DeepNLP/Transformer/P85-Non-Breaking-Prefix.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Udemy Lectures/DeepNLP/Transformer/P85-Non-Breaking-Prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6qds-ENwBt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\" 2. Cleaning the data \n",
        "\n",
        "I will split them in order to have the clean list of words and add space before each of them\n",
        "\n",
        "and dot after each of them because It will helps us to clean the data properly \"\"\"\n",
        "\n",
        "\n",
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izGOhXyrxJun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 3. I will remove the points which are not full stop, I am removing unwnated points or dots to make it\n",
        "easier for our algorithm to understand what really is it, Let's say we got some sentence where we have class starts\n",
        "\n",
        "at 2 A.M in such cases, I will remove it and I will store it into a new corpus,\n",
        "\n",
        "I will write the function to find the dots or points which are not at the end of the sentence\"\"\"\n",
        "\n",
        "\n",
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "\"\"\" Now I will check for the dots which are not followed by the space\n",
        "        If the full stop is followed by any number of letters then it is not considered Full stop \n",
        "        Here I will start with re.sub then type of string, We are looking for,\n",
        "        1. We will add points but will add backslash because point has a specific meaning in REGEX\n",
        "        2. Then Any sets of letters or number which is right after it and This one begins with ? = which means\n",
        "        it has to follow the points , Here = Means look for the letters or numbers but do not replace it\n",
        "        3. As I am looking for any number hence 0-9 \n",
        "        4. or , Please note , In REGEX , or is represented by | (vertical bar)\n",
        "        5. any letters small or capital hence written a-z | A-Z \n",
        "        6. But our aim is to replace the points only hence will mention here only consider the points for the\n",
        "           replacement part\n",
        "        7. Then at the end , I will specify the string, I would like to apply to, In my case here it is \n",
        "            corpus eng hence put that here \n",
        "            \n",
        "         NOw here we are I have selected all the points which i do not want to be ending points\n",
        "         Let's remove those points\"\"\"\n",
        "\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "\n",
        "\"\"\" Second regex is for removing these points or get replaced with nothing because we want to remove them\"\"\"\n",
        "\n",
        "\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "\n",
        "\"\"\" Now There is posibility that There could be many white spaces \n",
        "1. I am saying to look for white space followed by white space + (LOok closely I have given two spaces) \n",
        "2. I am writing to say that I want to replace all those whites spaces with a single white space(Look I have given\n",
        "    one single white spaces and at the end our corpus which is corpus_En \"\"\"\n",
        "\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "\n",
        "\"\"\" NOw Let's split the corpus english with respect blacklash and n \"\"\"\n",
        "\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "\"\"\" Now We will do the same with french corpus too \"\"\"\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdylkjXXzmES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 4. Tokenizing the text, Tokenizing is the process of transforming a set of chracters into one corresponding\n",
        "\n",
        "number or token\n",
        "\n",
        "Here i will tfds library to tokenize the word \n",
        "\n",
        "Note : It is always advisible to have lower number of target_vocab_size, It helps in improving the performance\n",
        "\n",
        "of transformer \"\"\"\n",
        "\n",
        "\n",
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "\n",
        "\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VG_OYebestx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" I will create two GLobal variable which I will use later\n",
        "1. Vocab size for English Language, \n",
        "\n",
        "I am giving value 2 here with plus sign because these are the words which will be added which means 2 words\n",
        "\n",
        "will be added but they are really words, They are just Homemade, This will be token which will be added at the\n",
        "\n",
        "begining of the sentence and other one will be added at the end of the sentence, As I said before, They are not\n",
        "\n",
        "really words but will be considered for the better of our model and Will do the same thing for french language\"\"\"\n",
        "\n",
        "\n",
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8190\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2 # = 8171\n",
        "\n",
        "\n",
        "\"\"\" Here I will create the real inputs and outputs to our model, Input will be a list of sentence, Each \n",
        "\n",
        "sentence being a sequence of numbers corressponding to the encoding of words and I will use starting and ending tokens\n",
        "so each sentence will start with the starting token that will be vocab size -2  which is exactly the vocab size of \n",
        "\n",
        "original tokenizer,\n",
        "\n",
        "Tokenizer_en_encode  is the string sentence into a list of numbers according to the tokenizing creation that\n",
        "\n",
        "I have already found. So I encode a sentence and add the end of our homemade ending token vocab size english-1\n",
        "\n",
        "and This is for every sentence in our original corpus,\n",
        "\n",
        "Now input is just a list of encoded sentences and will do the same with other language,\n",
        "\n",
        "Our second langauge is the OUTPUT Because We are building here a translator \"\"\"\n",
        "\n",
        "\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DKNvT1_jMk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 5. IF you will look at our dataset, It has some really long sentence and this is the issue here is why\n",
        "\n",
        "1. First one is when we will pads all sentences, So want to make all sentences the same lines for the batch to work\n",
        "\n",
        "2. It will take long time to train if we keep the really sentences and We reallu don't need to have those long\n",
        "\n",
        "sentences But IN case we are building a translator and have high power Machine, We can set the long sentences\n",
        "\n",
        "\n",
        "Here I am giving maximum length to the sentence as 20,\n",
        "\n",
        "Well! That seems quite low dataset\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "\"\"\" Creating a variable which will contain all the indices we want to get rid of \n",
        "\n",
        "Enumerate function is same as going through all the inputs but instead of just giving the sentences once at a time,\n",
        "\n",
        "it gives the sentences and counts so first element will zero and the sentence second sentences will give one and so on\n",
        "\"\"\"\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "\n",
        "\"\"\" Here I am doing reverse element, Now let's undertand why am i doing reverse element, let's say we want to remove\n",
        "first and second element, Once we have removed second elements all the indices of the element after those one would be shifted\n",
        "by one because these one would be  will have disappeared hence we need to process in the reverse order so that we are removing right\n",
        "elements\n",
        "\n",
        "And there I have used dell function which allows us to remove elements according to their indices right here\n",
        "and I will do it for input and output because I want to match every elements in input with output\n",
        "\n",
        "OKay So now I Have removed all the sentences from english that were longer than 20 words\n",
        "and Will do the same for french too\"\"\"\n",
        "\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbDW6B-gFp6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Input/ Output creation :\n",
        "\n",
        "\n",
        "1. We will pad all our input and output which means we want to make each sentence of same length \n",
        "\n",
        "2. Want to create dataset to shuffle back and do things which are neccessory\n",
        "\n",
        "I am padding POST which means I am padding zero at the end of each sentence  and Maxlength will tell How long\n",
        "\n",
        "our sentences will be \n",
        "\n",
        "Will do the same thing with output  \"\"\"\n",
        "\n",
        "\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)\n",
        "\n",
        "\n",
        "\"\"\" Now I will create the Dataset\n",
        "\n",
        "Will decide the Batch_size, We want to use- I will take here 64 as our batch size, But later we can change it\n",
        "\n",
        "Shuffle siz = 20K\"\"\"\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "\"\"\" NOw WE have dataset but in order to make it fully usefull or optimize it  by our training phase, WIll use \n",
        "\n",
        "first dataset cache- This helps in improving the way data is stored, They way we can have access to the data\n",
        "\n",
        "during the training, It increases the speed of the training but has nothing to do with performance of the model\n",
        "\n",
        "NOte : It is not mandatory to use hence you can ignore it, \n",
        "\n",
        "Dataset.prefetch is also used to get the training faster \"\"\"\n",
        "\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2HDn6Z4JwmM",
        "colab_type": "text"
      },
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o_7aV3kKkom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" This is very important part of Transformer , Now Will start building the transformer model\n",
        "\n",
        "I will not write here my own embedding because it is already available here But will create a customer layer\n",
        "\n",
        "in order to process this positional encoding that they explained in the Official paper , \n",
        "\n",
        "How positional encoding works , As I will be creating a custom layer hence it is going to be a class\n",
        "\n",
        "\n",
        "Here I am creating a class positionalencoding which will be inherit from layers.layer class\"\"\"\n",
        "\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "\n",
        "    \"\"\" Whenever we are creating a custom layer or class we have to define a init method which is, each time you\n",
        "    create an object from this class, It will take it as an agurments \"Self\" just like any other method, \n",
        "    This is the layer which will make only mathematical computation hence won't need any more parameters \n",
        "    because there will no trainable weights, No sub layers or anything this kind at this position encoding phase\n",
        "    SO This init method will only call super positional encoding\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" This line will call the init function of the class layer which has been well called the right there\n",
        "        so that the object can be built correctly with all the features that any layer can have\"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \"\"\" Now I will create another method that will perform the operation which is inside the sin and cosine function\n",
        "    to understand that You can visit the official paper of Transformer \n",
        "    SO  here defining this method get_angel which accepts the parameters like\n",
        "    1. self\n",
        "    2. pos\n",
        "    3. i\n",
        "    4. d_model \n",
        "    Here I am just writing the formula which is used in transformer for positional encoding and will add it\n",
        "    to our input \"\"\"\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRkmjsV7T7jy",
        "colab_type": "text"
      },
      "source": [
        "# Attention Machanism "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKqLYBpiT0c4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 1. Scaled Dot product Attention  \n",
        "\n",
        "NOte : If you want to understand how am i writing this, GO through the Transformer Research Paper\n",
        "\n",
        "Here Mask's work is that it does not allow decoder to see the next word\"\"\"\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Hv5hpBiR5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e16464f2-bf99-41be-9188-8e1f431352bb"
      },
      "source": [
        "\"\"\" Multi Head Attention Machanism\n",
        "\n",
        "Look at the architecture in the Research paper \n",
        "\n",
        "Here I will apply a linear function which is exactly a dense layer, Then SPlit the results into sub spaces\n",
        "\n",
        "For Each of those sub spaces, I Will apply the scale dot product attention And then I Will concatinate it \n",
        "\n",
        "so that we can get back to the words that have the right dimension which is the model\n",
        "\n",
        "And then Finally I will apply this last linear function to the whole computation \"\"\"\n",
        "\n",
        "\"\"\" Here I am writing a class which will be inherits from layers.layers And It will take init python function which will accept self,\n",
        "     And Number of Projection we want\"\"\"\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_proj):\n",
        "\n",
        "\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "\n",
        "\"\"\" What does this Build method do is actually it is same as class but instead of being called when we create the object\n",
        "but It is called when use the object for the first time , It kinds of completes the whole initialisation phase but it is has much more information\n",
        "\n",
        "An input_shape is something which we can always use in this build method , Input_shape is just the list which conveys the first input we call\"\"\"\n",
        "  \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        def split_proj(self, inputs, batch_size):\n",
        "           # inputs: (batch_size, seq_length, d_model)\n",
        "          shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "          splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "          return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "\n",
        " \"\"\" Now I will write the core method to just write the whole process or Whole architecture of sub layer and each time  we need a new layer \"\"\"\n",
        "\n",
        "         def call(self, queries, keys, values, mask):\n",
        "\n",
        "           batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "           queries = self.query_lin(queries)\n",
        "           keys = self.key_lin(keys)\n",
        "           values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-cb13cb424b92>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    def build(self, input_shape):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZLMoHp7KUkd",
        "colab_type": "text"
      },
      "source": [
        "# Encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QApPdmGxKTsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Let's look at the plan or steps I Will be performing to build our first encoder\n",
        "\n",
        "1. Apply multi head attention\n",
        "\n",
        "2. And then Add the result to the input and\n",
        "\n",
        "3. Apply some regular linear function USING THE dense layer and again use these small add and known sequence at the end \"\"\"\n",
        "\n",
        "\n",
        "\"\"\" Now We will start by class EncoderLayer\n",
        "\n",
        "Then Define the __init__ function, self as usual \n",
        "\n",
        "And As input We will have is Feed Forward units, This Feed forward units are number of units which I will use in neural network\n",
        "\n",
        "Number of Projections : It is for the Attention machanism\n",
        "And then Dropout Rate that I will to control the overfitting \n",
        "\n",
        "then we call to Super which will be applied to encoder layer and self and call the init Method from it. \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" I will write the build method for dimension of the models and things when we create the object at encoder layer\"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "\"\"\" Here I am aplying multi head attention to the input with number of projection\n",
        "    Will add some Drop out and normalization layer with few Dense layer, It is good to go through the Model's architecture while writing code for this\"\"\"\n",
        "\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}